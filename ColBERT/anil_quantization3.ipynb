{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "386bd935",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">An Exception was encountered at '<a href=\"#papermill-error-cell\">In [8]</a>'.</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a0de9b1c",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-01T22:47:44.013443Z",
     "iopub.status.busy": "2023-04-01T22:47:44.013031Z",
     "iopub.status.idle": "2023-04-01T22:47:45.136890Z",
     "shell.execute_reply": "2023-04-01T22:47:45.136315Z"
    },
    "papermill": {
     "duration": 1.129283,
     "end_time": "2023-04-01T22:47:45.138605",
     "exception": false,
     "start_time": "2023-04-01T22:47:44.009322",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import os\n",
    "import datetime\n",
    "import gc\n",
    "import torch.quantization\n",
    "from ptflops import get_model_complexity_info\n",
    "\n",
    "\n",
    "def timestamp():\n",
    "    print(datetime.datetime.now().strftime(\"%b %d %Y, %H:%M:%S\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4250278f",
   "metadata": {
    "papermill": {
     "duration": 0.002152,
     "end_time": "2023-04-01T22:47:45.143323",
     "exception": false,
     "start_time": "2023-04-01T22:47:45.141171",
     "status": "completed"
    },
    "tags": []
   },
   "source": [
    "# Prune Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "0172ac82",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-01T22:47:45.148499Z",
     "iopub.status.busy": "2023-04-01T22:47:45.148190Z",
     "iopub.status.idle": "2023-04-01T22:47:46.020961Z",
     "shell.execute_reply": "2023-04-01T22:47:46.020229Z"
    },
    "papermill": {
     "duration": 0.877277,
     "end_time": "2023-04-01T22:47:46.022655",
     "exception": false,
     "start_time": "2023-04-01T22:47:45.145378",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "from transformers import pipeline, AutoConfig\n",
    "from colbert.modeling.colbert import colbert_score\n",
    "from colbert.modeling.checkpoint import Checkpoint\n",
    "from colbert.infra import Run, RunConfig, ColBERTConfig\n",
    "from colbert import Trainer, Indexer, Searcher\n",
    "from transformers import AutoTokenizer\n",
    "from colbert.data import Queries\n",
    "import torch\n",
    "from tqdm.auto import tqdm\n",
    "import re\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "5bdc040b",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-01T22:47:46.028455Z",
     "iopub.status.busy": "2023-04-01T22:47:46.028151Z",
     "iopub.status.idle": "2023-04-01T22:47:46.032422Z",
     "shell.execute_reply": "2023-04-01T22:47:46.031937Z"
    },
    "papermill": {
     "duration": 0.008621,
     "end_time": "2023-04-01T22:47:46.033690",
     "exception": false,
     "start_time": "2023-04-01T22:47:46.025069",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def filter_layers(name, prune_type, ignore_bias=True):\n",
    "    if name.startswith('model.bert.embeddings') \\\n",
    "        or 'LayerNorm' in name: \n",
    "            return True\n",
    "    if ignore_bias and name.endswith('bias'):\n",
    "        return True\n",
    "    if prune_type == \"dense\":\n",
    "        if \"attention\" in name:\n",
    "            return True\n",
    "    elif \"attention\" in prune_type:\n",
    "        if \"attention\" not in name:\n",
    "            return True\n",
    "        if \"no_dense\" in prune_type and \"dense\" in name:\n",
    "            return True\n",
    "    return False"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "fda095ed",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-01T22:47:46.039112Z",
     "iopub.status.busy": "2023-04-01T22:47:46.038812Z",
     "iopub.status.idle": "2023-04-01T22:47:46.054218Z",
     "shell.execute_reply": "2023-04-01T22:47:46.053730Z"
    },
    "papermill": {
     "duration": 0.019627,
     "end_time": "2023-04-01T22:47:46.055435",
     "exception": false,
     "start_time": "2023-04-01T22:47:46.035808",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def quantization_data_new(config, quant_type, quant_Int):\n",
    "    use_iter = \"v2.0\"\n",
    "    \n",
    "    use_full_data = False\n",
    "    nbits = 2\n",
    "    k = 1000\n",
    "    maxsteps = 10000\n",
    "\n",
    "    base_path = fr\"experiments/\"\n",
    "    maxsteps_str=f\"10.000\"\n",
    "\n",
    "    base_path = fr\"experiments/\"\n",
    "    maxsteps_str=f\"{maxsteps:,}\".replace(',','.')\n",
    "    experiment = f\"msmarco_{maxsteps_str}\"\n",
    "    if use_full_data:\n",
    "        experiment += f\".data=full\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    #use_iter_str=f\"{use_iter:,}\".replace(',','.')\n",
    "    index_name = f\"msmarco_{maxsteps_str}{'.data=full' if use_full_data else ''}.nbits={nbits}\"\n",
    "\n",
    "\n",
    "    checkpoint = fr\"experiments/model_dump/colbert{use_iter}\" \n",
    "    retrieval_name = f\"{index_name}.ranking={k}.tsv\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "    if not os.path.exists(checkpoint):\n",
    "        #anil checkpoint = fr\"{base_path}/checkpoints/colbert\"\n",
    "        print(f\"Couldn't find checkpoint. Using default checkpoint: {checkpoint}\")\n",
    "        checkpoint = fr\"experiments/model_dump/colbertv2.0\"\n",
    "\n",
    "    config = ColBERTConfig(\n",
    "        bsize = 64,\n",
    "        root=base_path,\n",
    "        experiment=experiment,\n",
    "\n",
    "\n",
    "        #anil triples=r\"../data/triples.train.small.id.json\",\n",
    "        #anil collection= r\"../data/collection.tsv\",\n",
    "        triples=r\"../kngo/data/triples.train.small.id.json\",\n",
    "        collection= r\"../kngo/data/collection.tsv\",\n",
    "\n",
    "        checkpoint=checkpoint,\n",
    "        nbits=nbits,\n",
    "        overwrite='resume',\n",
    "        index_name=index_name,\n",
    "        index_path=fr\"/home/ubuntu/capstone/colbert/experiments/indexes/msmarco_10.000.nbits=2\",\n",
    "\n",
    "        rank = 0,\n",
    "        nranks = 1,\n",
    "        amp = True,\n",
    "        gpus = 1,\n",
    "    )\n",
    "\n",
    "    print(\"index_name=\",index_name)\n",
    "        \n",
    "    for q_type in quant_type:\n",
    "        print(f\"pruning model on prune type {q_type} to: {quant_Int}\")\n",
    "        with Run().context(RunConfig(nranks=config.nranks, experiment=config.experiment)):\n",
    "            model = Checkpoint(config.checkpoint, colbert_config=config)\n",
    "\n",
    "        #device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        #model.to(device)\n",
    "        model_state_dict = model.state_dict()\n",
    "        #for key, weight in model_state_dict.items():\n",
    "        #    print(\"key1=\",key,\",weight1 = \",weight)\n",
    "        quantized_model = torch.quantization.quantize_dynamic(model,q_type , dtype=quant_Int)\n",
    "        #quantized_model.save(\"pegasus-quantized-config\")\n",
    "        #model.config.save_pretrained(\"pegasus-quantized-config\")\n",
    "        #quantized_model.model.save_pretrained(\"pegasus-quantized-config\")\n",
    "        quantized_state_dict = quantized_model.state_dict()\n",
    "        #quantized_state_dict1 = {key.replace('model.', ''): quantized_state_dict.pop(key) for key in quantized_state_dict.keys()}\n",
    "        #torch.save(quantized_state_dict, \"pytorch_model.pt\")\n",
    "\n",
    "        #print(model)\n",
    "        #print(quantized_model)\n",
    "        #checkpoint = fr\"pegasus-quantized-config\"\n",
    "        #quantized_model.save(f\"lalal\")\n",
    "        def print_size_of_model(model):\n",
    "            torch.save(model.state_dict(), \"temp.p\")\n",
    "            print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "            os.remove('temp.p')\n",
    "            \n",
    "        #for key, weight in quantized_state_dict.items():\n",
    "        #    print(\"key2=\",key,\",weight2 = \",weight)\n",
    "       \n",
    "        print_size_of_model(model)\n",
    "        print_size_of_model(quantized_model)\n",
    "        \n",
    "        if do_retrieval:\n",
    "            timestamp()\n",
    "            gc.collect()\n",
    "            config.set(\"queries\", r\"../kngo/data/queries.dev.tsv\")\n",
    "            \n",
    "  \n",
    "            with Run().context(RunConfig(nranks=config.nranks, experiment=config.experiment, name='retrieval', overwrite = True)):\n",
    "                \n",
    "                config.checkpoint = model\n",
    "                model.to('cpu')\n",
    "                searcher = Searcher(index=config.index_name, config=config, checkpoint=model)\n",
    "                queries = Queries(config.queries)\n",
    "                count = 0\n",
    "                while(count !=10):\n",
    "                    print(f\"Base model #\", count)\n",
    "                    ranking = searcher.search_all(queries, k=k)\n",
    "                    count = count + 1\n",
    "                #ranking.save(f\"msmarco.{use_iter}.nbits={config.nbits}.prune={prune_amount}.prune_type={prune_type}.ranking={k}.tsv\")\n",
    "                #ranking.save(retrieval_name)\n",
    "            timestamp()\n",
    "\n",
    "            del searcher, queries, ranking\n",
    "            gc.collect()\n",
    "            \n",
    "            with Run().context(RunConfig(nranks=config.nranks, experiment=config.experiment, name='retrieval', overwrite = True)):\n",
    "                \n",
    "                config.checkpoint = quantized_model\n",
    "                quantized_model.to('cpu')\n",
    "                searcher = Searcher(index=config.index_name, config=config, checkpoint=quantized_model)\n",
    "                queries = Queries(config.queries)\n",
    "                count = 0\n",
    "                while(count !=10):\n",
    "                    print(f\"Quantized model #\", count)\n",
    "                    ranking = searcher.search_all(queries, k=k)\n",
    "                    count = count + 1\n",
    "                #ranking.save(f\"msmarco.{use_iter}.nbits={config.nbits}.prune={prune_amount}.prune_type={prune_type}.ranking={k}.tsv\")\n",
    "                #ranking.save(retrieval_name)\n",
    "            timestamp()\n",
    "\n",
    "            del searcher, queries, ranking\n",
    "            gc.collect()\n",
    "             \n",
    "\n",
    "        if do_eval:\n",
    "            #!python -m utility.evaluate.msmarco_passages \\\n",
    "            #     --ranking \"experiments/msmarco_{maxsteps_str}/retrieval/msmarco.{use_iter}.nbits={config.nbits}.prune={prune_amount}.prune_type={prune_type}.ranking={k}.tsv\" \\\n",
    "            #     --qrels \"../data/qrels.dev.tsv\" > \"experiments/msmarco_{maxsteps_str}/retrieval/msmarco.{use_iter}.nbits={config.nbits}.prune={prune_amount}.prune_type={prune_type}.ranking={k}.tsv.log\"\n",
    "            !python -m utility.evaluate.msmarco_passages \\\n",
    "                --ranking \"experiments/{experiment}/none/retrieval/{retrieval_name}\" \\\n",
    "                --qrels \"../kngo/data/qrels.dev.tsv\" #> \"experiments/{experiment}/retrieval/{retrieval_name}.log\"\n",
    "        del model,quantized_model\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56868123",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-01T22:47:46.060671Z",
     "iopub.status.busy": "2023-04-01T22:47:46.060344Z",
     "iopub.status.idle": "2023-04-01T22:47:46.064450Z",
     "shell.execute_reply": "2023-04-01T22:47:46.063968Z"
    },
    "papermill": {
     "duration": 0.008156,
     "end_time": "2023-04-01T22:47:46.065733",
     "exception": false,
     "start_time": "2023-04-01T22:47:46.057577",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "def quantization_data(config, quant_type, quant_Int):\n",
    "\n",
    "    for q_type in quant_type:\n",
    "        print(f\"pruning model on prune type {q_type} to: {quant_Int}\")\n",
    "        with Run().context(RunConfig(nranks=config.nranks, experiment=config.experiment)):\n",
    "            model = Checkpoint(config.checkpoint, colbert_config=config)\n",
    "\n",
    "        device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "        model.to(device)\n",
    "        quantized_model = torch.quantization.quantize_dynamic(model,q_type , dtype=quant_Int)\n",
    "        quantized_model.save(f\"{checkpoint}.quant={quant_Int}.quant_type={q_type}\")\n",
    "        del model,quantized_model\n",
    "        gc.collect()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "73390957",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-01T22:47:46.071026Z",
     "iopub.status.busy": "2023-04-01T22:47:46.070721Z",
     "iopub.status.idle": "2023-04-01T22:47:46.073361Z",
     "shell.execute_reply": "2023-04-01T22:47:46.072875Z"
    },
    "papermill": {
     "duration": 0.00665,
     "end_time": "2023-04-01T22:47:46.074594",
     "exception": false,
     "start_time": "2023-04-01T22:47:46.067944",
     "status": "completed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "\n",
    "quantization_Int = [torch.qint8]\n",
    "quantization_Type = [{torch.nn.Linear}]\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41943553",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-01T22:47:46.079925Z",
     "iopub.status.busy": "2023-04-01T22:47:46.079625Z",
     "iopub.status.idle": "2023-04-02T00:10:24.080386Z",
     "shell.execute_reply": "2023-04-02T00:10:24.079725Z"
    },
    "papermill": {
     "duration": 4959.330719,
     "end_time": "2023-04-02T00:10:25.407498",
     "exception": false,
     "start_time": "2023-04-01T22:47:46.076779",
     "status": "completed"
    },
    "scrolled": false,
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "index_name= msmarco_10.000.nbits=2\n",
      "pruning model on prune type {<class 'torch.nn.modules.linear.Linear'>} to: torch.qint8\n",
      "[Apr 06, 03:04:51] [0] \t\t Option1\n",
      "\n",
      "model type = ['HF_ColBERT']\n",
      "creating a HF_ColBERT model\n",
      "We are loading the model1\n",
      "We are loading the model2\n",
      "Size (MB): 438.393806\n",
      "Size (MB): 181.584042\n",
      "Apr 06 2023, 03:04:55\n",
      "[Apr 06, 03:04:55] [0] \t\t Option1\n",
      "\n",
      "/home/ubuntu/capstone/colbert/experiments/msmarco_10.000/indexes/ msmarco_10.000.nbits=2\n",
      "[Apr 06, 03:04:56] #> Loading collection...\n",
      "0M 1M 2M 3M 4M 5M 6M 7M 8M \n",
      "[Apr 06, 03:05:10] #> Loading codec...\n",
      "[Apr 06, 03:05:10] Loading decompress_residuals_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Apr 06, 03:05:11] Loading packbits_cpp extension (set COLBERT_LOAD_TORCH_EXTENSION_VERBOSE=True for more info)...\n",
      "[Apr 06, 03:05:13] #> Loading IVF...\n",
      "[Apr 06, 03:05:14] #> Loading doclens...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|█████████████████████████████████████████████████████████████████████████████████████████████████████████| 354/354 [00:00<00:00, 1213.98it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Apr 06, 03:05:16] #> Loading codes and residuals...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████████████████████████████████| 354/354 [00:14<00:00, 24.53it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[Apr 06, 03:05:30] #> Loading the queries from ../kngo/data/queries.dev.tsv ...\n",
      "[Apr 06, 03:05:30] #> Got 101093 queries. All QIDs are unique.\n",
      "\n",
      "Base model # 0\n",
      "Encoding Start\n",
      "Apr 06 2023, 03:05:30\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#if not os.path.exists(checkpoint):\n",
    "#    checkpoint = fr\"{base_path}/checkpoints/colbert\"\n",
    "base_path = fr\"experiments/\"\n",
    "checkpoint = fr\"experiments/model_dump/colbertv2.0\"\n",
    "\n",
    "do_retrieval = True\n",
    "do_eval = True\n",
    "    \n",
    "config = ColBERTConfig(\n",
    "        bsize = 64,\n",
    "        root=base_path,\n",
    "    \n",
    "        triples=r\"../kngo/data/triples.train.small.id.json\",\n",
    "        collection= r\"../kngo/data/collection.tsv\",\n",
    "        \n",
    "        checkpoint = checkpoint,\n",
    "        overwrite='resume',\n",
    "    \n",
    "        ncells= 10,\n",
    "    \n",
    "        rank = 0,\n",
    "        nranks = 1,\n",
    "        amp = True,\n",
    "        gpus = 1,\n",
    "    )\n",
    "\n",
    "for q_Int in quantization_Int:\n",
    "    quantization_data_new(config, quantization_Type, q_Int )\n",
    "print(\"quantization complete\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9da1ee1c",
   "metadata": {
    "tags": [
     "papermill-error-cell-tag"
    ]
   },
   "source": [
    "<span id=\"papermill-error-cell\" style=\"color:red; font-family:Helvetica Neue, Helvetica, Arial, sans-serif; font-size:2em;\">Execution using papermill encountered an exception here and stopped:</span>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ae36cecb",
   "metadata": {
    "execution": {
     "iopub.execute_input": "2023-04-02T00:10:28.046794Z",
     "iopub.status.busy": "2023-04-02T00:10:28.046437Z",
     "iopub.status.idle": "2023-04-02T00:10:28.184308Z",
     "shell.execute_reply": "2023-04-02T00:10:28.183593Z"
    },
    "papermill": {
     "duration": 1.456666,
     "end_time": "2023-04-02T00:10:28.185451",
     "exception": true,
     "start_time": "2023-04-02T00:10:26.728785",
     "status": "failed"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "for p_amount in prune_amount:\n",
    "    gc.collect()\n",
    "    prune_experiment(prune_type, p_amount, maxsteps = 10000,  k = 1000, \\\n",
    "                     do_train = False, do_index = False, do_retrieval = False, do_eval = True, nbits = 2, \\\n",
    "                     use_full_data = False)\n",
    "print(\"!!!!all done!!!!!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2fba3f5",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d3e68199",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "#input_ids = ids_tensor([8, 128], 2)\n",
    "#token_type_ids = ids_tensor([8, 128], 2)\n",
    "#attention_mask = ids_tensor([8, 128], vocab_size=2)\n",
    "#dummy_input = (input_ids, attention_mask, token_type_ids)\n",
    "#traced_model = torch.jit.trace(quantized_model, dummy_input)\n",
    "#torch.jit.save(traced_model, \"bert_traced_eager_quant.pt\")\n",
    "#print(\"Saved the model\")\n",
    "\n",
    "# save config\n",
    "#quantized_model.config.save_pretrained(\"pegasus-quantized-config\")\n",
    "# save state dict\n",
    "#quantized_state_dict = quantized_model.state_dict()\n",
    "#torch.save(quantized_state_dict, \"pegasus-quantized.pt\")\n",
    "\n",
    "\n",
    "# load config and dummy model\n",
    "#config = AutoConfig.from_pretrained(\"pegasus-quantized-config\")\n",
    "#dummy_model = PegasusForConditionalGeneration(config)\n",
    "#4. quantize dummy model and load state dict\n",
    "#reconstructed_quantized_model = torch.quantization.quantize_dynamic(\n",
    "#    dummy_model, {torch.nn.Linear}, dtype=torch.qint8\n",
    "#)\n",
    "#reconstructed_quantized_model.load_state_dict(quantized_state_dict)\n",
    "\n",
    "\n",
    "\n",
    "config.set(\"queries\", r\"../kngo/data/queries.dev_clean.tsv\" \n",
    "           if os.path.exists(r\"../kngo/data/queries.dev_clean.tsv\") \n",
    "           else r\"../kngo/data/queries.small.dev.tsv\")\n",
    "with Run().context(RunConfig(nranks=config.nranks, experiment=config.experiment, name='retrieval', overwrite = True)):\n",
    "    print(\"lala=\",config.index_name, quantized_model, retrieval_name, experiment)\n",
    "\n",
    "    searcher = Searcher(index=config.index_name, config=config, checkpoint=quantized_model)\n",
    "    queries = Queries(config.queries)\n",
    "    ranking = searcher.search_all(queries, k=k)\n",
    "    #ranking.save(f\"msmarco.{use_iter}.nbits={config.nbits}.prune={prune_amount}.prune_type={prune_type}.ranking={k}.tsv\")\n",
    "    ranking.save(retrieval_name)\n",
    "timestamp()\n",
    "\n",
    "del searcher, queries, ranking\n",
    "gc.collect()\n",
    "\n",
    "!python -m utility.evaluate.msmarco_passages \\\n",
    "    --ranking \"experiments/{experiment}/none/retrieval/{retrieval_name}\" \\\n",
    "    --qrels \"../kngo/data/qrels.dev.tsv\" #> \"experiments/{experiment}/retrieval/{retrieval_name}.log\"\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19aba086",
   "metadata": {
    "papermill": {
     "duration": null,
     "end_time": null,
     "exception": null,
     "start_time": null,
     "status": "pending"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import torch\n",
    "from transformers import AutoConfig\n",
    "#experimentsmsmarco_10.000indexesmsmarco_10.000.nbits=2\n",
    "\n",
    "use_full_data = False\n",
    "do_train = False\n",
    "do_index = False\n",
    "do_retrieval = False\n",
    "do_eval = True\n",
    "nbits = 2\n",
    "use_iter = \"v2.0\"\n",
    "k = 1000\n",
    "\n",
    "base_path = fr\"experiments/\"\n",
    "maxsteps_str=f\"10.000\"\n",
    "experiment = f\"msmarco_{maxsteps_str}\"\n",
    "if use_full_data:\n",
    "    experiment += f\".data=full\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "#use_iter_str=f\"{use_iter:,}\".replace(',','.')\n",
    "index_name = f\"msmarco_{maxsteps_str}{'.data=full' if use_full_data else ''}.nbits={nbits}\"\n",
    "\n",
    "\n",
    "checkpoint = fr\"experiments/model_dump/colbert{use_iter}\" \n",
    "retrieval_name = f\"{index_name}.ranking={k}.tsv\"\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "if not os.path.exists(checkpoint):\n",
    "    #anil checkpoint = fr\"{base_path}/checkpoints/colbert\"\n",
    "    print(f\"Couldn't find checkpoint. Using default checkpoint: {checkpoint}\")\n",
    "    checkpoint = fr\"experiments/model_dump/colbertv2.0\"\n",
    "\n",
    "config = ColBERTConfig(\n",
    "    bsize = 64,\n",
    "    root=base_path,\n",
    "    experiment=experiment,\n",
    "    name=experiment,\n",
    "\n",
    "\n",
    "    #anil triples=r\"../data/triples.train.small.id.json\",\n",
    "    #anil collection= r\"../data/collection.tsv\",\n",
    "    triples=r\"../kngo/data/triples.train.small.id.json\",\n",
    "    collection= r\"../kngo/data/collection.tsv\",\n",
    "\n",
    "    checkpoint=checkpoint,\n",
    "    nbits=nbits,\n",
    "    overwrite='resume',\n",
    "    index_name=index_name,\n",
    "    index_path=fr\"{base_path}indexes/{index_name}\",\n",
    "\n",
    "    rank = 0,\n",
    "    nranks = 1,\n",
    "    amp = True,\n",
    "    gpus = 1,\n",
    ")\n",
    "    \n",
    "\n",
    "\n",
    "with Run().context(RunConfig(nranks=config.nranks, experiment=config.experiment)):\n",
    "    model = Checkpoint(config.checkpoint, colbert_config=config)\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "model.to(device)\n",
    "\n",
    "# quantize model\n",
    "quantized_model = torch.quantization.quantize_dynamic(\n",
    "    model, {torch.nn.Linear}, dtype=torch.qint8\n",
    ")\n",
    "\n",
    "def print_size_of_model(model):\n",
    "    torch.save(model.state_dict(), \"temp.p\")\n",
    "    print('Size (MB):', os.path.getsize(\"temp.p\")/1e6)\n",
    "    os.remove('temp.p')\n",
    "\n",
    "print_size_of_model(model)\n",
    "print_size_of_model(quantized_model)\n",
    "\n",
    "#quantized_model.model.save_pretrained(\"tmp-t5-small-quantized-config\")  # save config\n",
    "quantized_state_dict = quantized_model.state_dict()\n",
    "#for key, weight in quantized_state_dict.items():\n",
    "#    print(key,weight)\n",
    "#torch.jit.save(quantized_state_dict, \"tmp-t5-small-quantized-state-dict.pt\")\n",
    "torch.save(quantized_state_dict, \"tmp-t5-small-quantized-state-dict.pt\")\n",
    "\n",
    "# Transform your model into a quantized model\n",
    "quantized_model = torch.quantization.quantize_dynamic(model, {torch.nn.Linear}, dtype=torch.qint8)\n",
    "# Load the quantized weights into the quantized model (module in torch)\n",
    "quantized_model.load_state_dict(torch.load(\"tmp-t5-small-quantized-state-dict.pt\"))\n",
    "\n",
    "print('Load quantized model')\n",
    "#quantized_config = AutoConfig.from_pretrained(\"tmp-t5-small-quantized-config\")\n",
    "#dummy_model = ColBERT(quantized_config)\n",
    "\n",
    "#reconstructed_quantized_model = torch.quantization.quantize_dynamic(\n",
    "#    dummy_model, {torch.nn.Linear}, dtype=torch.qint8\n",
    "#)\n",
    "#reconstructed_quantized_model.load_state_dict(torch.load(\"tmp-t5-small-quantized-state-dict.pt\"))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d45ed2",
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import absolute_import, division, print_function\n",
    "\n",
    "import logging\n",
    "import numpy as np\n",
    "import os\n",
    "import random\n",
    "import sys\n",
    "import time\n",
    "import torch\n",
    "\n",
    "from argparse import Namespace\n",
    "from torch.utils.data import (DataLoader, RandomSampler, SequentialSampler,\n",
    "                              TensorDataset)\n",
    "from tqdm import tqdm\n",
    "from transformers import (BertConfig, BertForSequenceClassification, BertTokenizer,)\n",
    "from transformers import glue_compute_metrics as compute_metrics\n",
    "from transformers import glue_output_modes as output_modes\n",
    "from transformers import glue_processors as processors\n",
    "from transformers import glue_convert_examples_to_features as convert_examples_to_features\n",
    "from torch.quantization import per_channel_dynamic_qconfig\n",
    "from torch.quantization import quantize_dynamic_jit\n",
    "\n",
    "global_rng = random.Random()\n",
    "\n",
    "def ids_tensor(shape, vocab_size, rng=None, name=None):\n",
    "    #  Creates a random int32 tensor of the shape within the vocab size\n",
    "    if rng is None:\n",
    "        rng = global_rng\n",
    "\n",
    "    total_dims = 1\n",
    "    for dim in shape:\n",
    "        total_dims *= dim\n",
    "\n",
    "    values = []\n",
    "    for _ in range(total_dims):\n",
    "        values.append(rng.randint(0, vocab_size - 1))\n",
    "\n",
    "    return torch.tensor(data=values, dtype=torch.long, device='cpu').view(shape).contiguous()\n",
    "\n",
    "# Setup logging\n",
    "logger = logging.getLogger(__name__)\n",
    "logging.basicConfig(format = '%(asctime)s - %(levelname)s - %(name)s -   %(message)s',\n",
    "                    datefmt = '%m/%d/%Y %H:%M:%S',\n",
    "                    level = logging.WARN)\n",
    "\n",
    "logging.getLogger(\"transformers.modeling_utils\").setLevel(\n",
    "   logging.WARN)  # Reduce logging\n",
    "\n",
    "print(torch.__version__)\n",
    "\n",
    "torch.set_num_threads(1)\n",
    "print(torch.__config__.parallel_info())"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.16"
  },
  "papermill": {
   "default_parameters": {},
   "duration": 4969.626694,
   "end_time": "2023-04-02T00:10:32.479354",
   "environment_variables": {},
   "exception": true,
   "input_path": "anil_quantization.ipynb",
   "output_path": "anil_quantization_output1.ipynb",
   "parameters": {},
   "start_time": "2023-04-01T22:47:42.852660",
   "version": "2.4.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
